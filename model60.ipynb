{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitDarrwin/FYP/blob/main/model60.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Pv9Ztn1j5M9S",
        "outputId": "b2838d97-24f6-4b25-9e87-2252ffef383d"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c33a429bd6c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# prompt: connect to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fhVdmNpw5j_c"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "with tarfile.open('/content/drive/MyDrive/Colab Notebooks/Dataset for MALWARE/label_metadata.tar.gz', 'r:gz') as tar:\n",
        "  tar.extractall('/content/extracted_files/label') # Extract to a new directory\n",
        "\n",
        "with tarfile.open('/content/drive/MyDrive/Colab Notebooks/Dataset for MALWARE/malnet-graphs-tiny.tar.gz', 'r:gz') as tar:\n",
        "  tar.extractall('/content/extracted_files/malnet') # Extract to the same directory\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/Dataset for MALWARE/split_info_tiny.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/extracted_files/splitinfo') # Extract to the same directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P1tYcDGh6mG",
        "outputId": "e4817cbf-9786-4d90-9575-7ab62aaa8d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkYSI0ls2U0e",
        "outputId": "52168bef-fa76-43b5-9dee-5fc84ee2186d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "k-6ZuX4823l4",
        "outputId": "d8d8b998-44b6-4714-dd51-47d5068f79bb"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a0403d709ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_mean_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# from torch.optim import AdamW\n",
        "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "# from tqdm import tqdm\n",
        "# import networkx as nx\n",
        "# from torch_geometric.data import Data, DataLoader\n",
        "# from torch_geometric.nn import GATConv, global_mean_pool\n",
        "# from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.manifold import TSNE\n",
        "# import yt_dlp\n",
        "# import IPython.display as ipd\n",
        "\n",
        "# def play_youtube_audio(url):\n",
        "#     ydl_opts = {\n",
        "#         'format': 'bestaudio/best',\n",
        "#         'postprocessors': [{\n",
        "#             'key': 'FFmpegExtractAudio',\n",
        "#             'preferredcodec': 'mp3',\n",
        "#             'preferredquality': '192',\n",
        "#         }],\n",
        "#         'outtmpl': 'audio.%(ext)s'\n",
        "#     }\n",
        "#     with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "#         ydl.download([url])\n",
        "#     return ipd.Audio(\"audio.mp3\", autoplay=True)\n",
        "\n",
        "# # --- Set seeds and device ---\n",
        "# SEED = 75\n",
        "# random.seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.manual_seed_all(SEED)\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "#     # Enable benchmark for faster training when possible\n",
        "#     torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# # --- Label and path definitions ---\n",
        "# LABEL_MAPPING = {\n",
        "#     'adware': 0,\n",
        "#     'benign': 1,\n",
        "#     'downloader': 2,\n",
        "#     'trojan': 3,\n",
        "#     'addisplay': 4  # This class will be treated as unseen during training\n",
        "# }\n",
        "\n",
        "# # Set file paths (modify if needed)\n",
        "# graph_data_path = '/content/extracted_files/malnet/malnet-graphs-tiny'\n",
        "# split_info_path = '/content/extracted_files/splitinfo/split_info_tiny/type'\n",
        "\n",
        "# # --- Split load helper ---\n",
        "# def load_split(file_name):\n",
        "#     split_file = os.path.join(split_info_path, file_name)\n",
        "#     with open(split_file, 'r') as f:\n",
        "#         return [line.strip() for line in f]\n",
        "\n",
        "# train_graphs = load_split('train.txt')\n",
        "# val_graphs = load_split('val.txt')\n",
        "# test_graphs = load_split('test.txt')\n",
        "\n",
        "# # --- Node feature calculation ---\n",
        "# @torch.no_grad()\n",
        "# def calculate_node_features(G_nx, num_nodes):\n",
        "#     # Existing features\n",
        "#     degrees = torch.tensor([d for _, d in G_nx.degree()], dtype=torch.float32)\n",
        "#     G_undirected = G_nx.to_undirected()\n",
        "#     clustering_dict = nx.clustering(G_undirected)\n",
        "#     clustering_values = torch.tensor([clustering_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "#     # Calculate additional centrality metrics\n",
        "#     # For larger graphs, use approximation to improve speed\n",
        "#     if num_nodes > 500:\n",
        "#         betweenness_dict = nx.betweenness_centrality(G_nx, k=min(50, num_nodes))\n",
        "#         closeness_dict = {i: 0.5 for i in range(num_nodes)}  # Default value for large graphs\n",
        "#     else:\n",
        "#         betweenness_dict = nx.betweenness_centrality(G_nx)\n",
        "#         try:\n",
        "#             closeness_dict = nx.closeness_centrality(G_nx)\n",
        "#         except:\n",
        "#             closeness_dict = {i: 0.5 for i in range(num_nodes)}\n",
        "\n",
        "#     betweenness_values = torch.tensor([betweenness_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "#     closeness_values = torch.tensor([closeness_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "#     # Calculate eigenvector centrality (important for capturing influence patterns)\n",
        "#     try:\n",
        "#         eigenvector_dict = nx.eigenvector_centrality(G_nx, max_iter=100)\n",
        "#     except Exception as e:\n",
        "#         eigenvector_dict = {i: 1.0 / num_nodes for i in range(num_nodes)}\n",
        "#     eigenvector_values = torch.tensor([eigenvector_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "#     # Calculate pagerank (useful for identifying important nodes)\n",
        "#     try:\n",
        "#         pagerank_dict = nx.pagerank(G_nx, max_iter=100)\n",
        "#     except:\n",
        "#         pagerank_dict = {i: 1.0 / num_nodes for i in range(num_nodes)}\n",
        "#     pagerank_values = torch.tensor([pagerank_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "#     # Include graph density feature (trojans often have distinct density patterns)\n",
        "#     graph_density = nx.density(G_nx)\n",
        "#     density_feature = torch.full((num_nodes, 1), graph_density, dtype=torch.float32)\n",
        "\n",
        "#     return torch.cat([\n",
        "#         degrees.unsqueeze(1),\n",
        "#         clustering_values.unsqueeze(1),\n",
        "#         betweenness_values.unsqueeze(1),\n",
        "#         eigenvector_values.unsqueeze(1),\n",
        "#         closeness_values.unsqueeze(1),\n",
        "#         pagerank_values.unsqueeze(1),\n",
        "#         density_feature\n",
        "#     ], dim=1)\n",
        "\n",
        "# # --- Graph loading function with improved error handling ---\n",
        "# def load_graphs(graph_list, desc=\"Loading graphs\"):\n",
        "#     data_list = []\n",
        "#     with tqdm(total=len(graph_list), desc=desc, position=0, leave=True) as pbar:\n",
        "#         for graph_entry in graph_list:\n",
        "#             # Expecting \"class/filename\" structure\n",
        "#             category_folder, filename = os.path.split(graph_entry)\n",
        "#             class_name = category_folder.split('/')[0]\n",
        "#             if class_name not in LABEL_MAPPING:\n",
        "#                 pbar.update(1)\n",
        "#                 continue\n",
        "\n",
        "#             graph_path = os.path.join(graph_data_path, category_folder, f\"{filename}.edgelist\")\n",
        "#             if not os.path.exists(graph_path):\n",
        "#                 pbar.update(1)\n",
        "#                 continue\n",
        "\n",
        "#             try:\n",
        "#                 # Efficient edge list reading\n",
        "#                 with open(graph_path, 'r') as f:\n",
        "#                     edge_list = [tuple(map(int, line.strip().split()))\n",
        "#                                  for line in f if line.strip() and not line.startswith('#')]\n",
        "#                 if not edge_list:\n",
        "#                     pbar.update(1)\n",
        "#                     continue\n",
        "\n",
        "#                 edge_index = torch.tensor(edge_list).t().contiguous()\n",
        "#                 num_nodes = int(edge_index.max().item()) + 1\n",
        "\n",
        "#                 # Build networkx graph and calculate node features\n",
        "#                 G_nx = nx.DiGraph()\n",
        "#                 G_nx.add_nodes_from(range(num_nodes))\n",
        "#                 G_nx.add_edges_from(edge_list)\n",
        "#                 x = calculate_node_features(G_nx, num_nodes)\n",
        "\n",
        "#                 data = Data(\n",
        "#                     x=x,\n",
        "#                     edge_index=edge_index,\n",
        "#                     y=torch.tensor([LABEL_MAPPING[class_name]], dtype=torch.long)\n",
        "#                 )\n",
        "#                 data_list.append(data)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"[Warning] Error processing {graph_path}: {str(e)}\")\n",
        "#             finally:\n",
        "#                 pbar.update(1)\n",
        "#     return data_list\n",
        "\n",
        "# def visualize_attributes(attribute_matrix):\n",
        "#     attr_vectors = attribute_matrix().detach().cpu().numpy()  # Detach the tensor to remove gradients\n",
        "#     tsne = TSNE(n_components=2, random_state=42, perplexity=3)  # Set perplexity to 3 or another value less than 5\n",
        "#     reduced = tsne.fit_transform(attr_vectors)\n",
        "#     plt.scatter(reduced[:, 0], reduced[:, 1], c=range(len(LABEL_MAPPING)), cmap='viridis')\n",
        "#     for i, label in enumerate(LABEL_MAPPING.keys()):\n",
        "#         plt.annotate(label, (reduced[i, 0], reduced[i, 1]))\n",
        "#     plt.title(\"t-SNE Visualization of Attribute Vectors\")\n",
        "#     plt.show()\n",
        "\n",
        "# # --- Load datasets ---\n",
        "# print(\"Loading datasets...\")\n",
        "# train_data_raw = load_graphs(train_graphs, desc=\"Training graphs\")\n",
        "# val_data_raw = load_graphs(val_graphs, desc=\"Validation graphs\")\n",
        "# test_data_raw = load_graphs(test_graphs, desc=\"Test graphs\")\n",
        "\n",
        "# # Filter out the unseen class ('addisplay') from training data\n",
        "# unseen_label = LABEL_MAPPING['addisplay']\n",
        "# train_data = [d for d in train_data_raw if d.y.item() != unseen_label]\n",
        "# val_data = val_data_raw\n",
        "# test_data = test_data_raw\n",
        "\n",
        "# from gensim.models import KeyedVectors\n",
        "\n",
        "# # --- Load GloVe embeddings (pre-converted to Word2Vec format) ---\n",
        "# glove_model = KeyedVectors.load_word2vec_format('/content/model.word2vec', binary=False)\n",
        "\n",
        "# # Otherwise, try loading directly (GloVe format):\n",
        "# # glove_model = KeyedVectors.load_word2vec_format('path/to/glove.6B.100d.txt', binary=False)\n",
        "# # --- Build a dictionary of pretrained embeddings for each label ---\n",
        "# pretrained_embeddings = {}\n",
        "# for label in LABEL_MAPPING.keys():\n",
        "#     if label in glove_model:\n",
        "#         pretrained_embeddings[label] = glove_model[label].tolist()\n",
        "#     else:\n",
        "#         # If the label isn't found, create a random 100-d vector.\n",
        "#         pretrained_embeddings[label] = [random.uniform(-0.1, 0.1) for _ in range(100)]\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn  # <-- add this at the top\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # --- Attribute matrix module ---\n",
        "# # --- Updated Attribute Matrix Module ---\n",
        "# class AttributeMatrix(nn.Module):\n",
        "#     def __init__(self, num_classes, input_dim, embed_dim, label_mapping, pretrained_embeddings):\n",
        "#         super().__init__()\n",
        "#         self.embed_dim = embed_dim\n",
        "\n",
        "#         # Enhanced projection network with more layers\n",
        "#         self.proj = nn.Sequential(\n",
        "#             nn.Linear(input_dim, input_dim),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Linear(input_dim, embed_dim),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Linear(embed_dim, embed_dim)\n",
        "#         )\n",
        "\n",
        "#         # Separate residual path\n",
        "#         self.residual = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "#         # Initialize embeddings with higher weight for trojan class\n",
        "#         embeddings_list = []\n",
        "#         for label, idx in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
        "#             if label in pretrained_embeddings:\n",
        "#                 vec = torch.tensor(pretrained_embeddings[label], dtype=torch.float32)\n",
        "\n",
        "#                 # Give more weight to the trojan class embeddings\n",
        "#                 if label == 'trojan':\n",
        "#                     vec = vec * 1.2  # Amplify trojan features\n",
        "\n",
        "#             else:\n",
        "#                 vec = torch.randn(input_dim)\n",
        "#             embeddings_list.append(vec)\n",
        "\n",
        "#         attributes_tensor = torch.stack(embeddings_list, dim=0)\n",
        "#         projected = self.proj(attributes_tensor)\n",
        "#         residual_out = self.residual(attributes_tensor)\n",
        "#         combined = projected + residual_out  # Residual connection\n",
        "\n",
        "#         # Initialize the attributes with the combined embeddings\n",
        "#         self.attributes = nn.Parameter(combined)\n",
        "\n",
        "#     def forward(self, indices=None):\n",
        "#         # Add L2 normalization for better cosine similarity calculation\n",
        "#         attr = F.normalize(self.attributes, p=2, dim=1)\n",
        "#         return attr if indices is None else attr[indices]\n",
        "\n",
        "# # --- Zero-Shot GNN Model ---\n",
        "# class ZSLGNN(nn.Module):\n",
        "#     def __init__(self, input_dim=7, hidden_dim=64, embed_dim=64, dropout_rate=0.3):\n",
        "#         super().__init__()\n",
        "#         # Increase attention heads for better feature attention\n",
        "#         self.conv1 = GATConv(input_dim, hidden_dim, heads=4, dropout=dropout_rate)\n",
        "#         self.conv2 = GATConv(hidden_dim * 4, hidden_dim, heads=4, dropout=dropout_rate)\n",
        "#         self.conv3 = GATConv(hidden_dim * 4, hidden_dim, heads=4, dropout=dropout_rate)\n",
        "\n",
        "#         # Add batch normalization for more stable training\n",
        "#         self.bn1 = nn.BatchNorm1d(hidden_dim * 4)\n",
        "#         self.bn2 = nn.BatchNorm1d(hidden_dim * 4)\n",
        "#         self.bn3 = nn.BatchNorm1d(hidden_dim * 4)\n",
        "\n",
        "#         # Enhanced MLP for final projection\n",
        "#         self.fc = nn.Sequential(\n",
        "#             nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(hidden_dim * 2, embed_dim),\n",
        "#             nn.LeakyReLU(0.2),\n",
        "#         )\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "#     def forward(self, data):\n",
        "#         x = data.x.to(device)\n",
        "#         edge_index = data.edge_index.to(device)\n",
        "#         batch = data.batch.to(device) if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "#         # Forward pass with skip connections and batch normalization\n",
        "#         x1 = self.dropout(F.elu(self.conv1(x, edge_index)))\n",
        "#         x1 = self.bn1(x1)\n",
        "\n",
        "#         x2 = self.dropout(F.elu(self.conv2(x1, edge_index)))\n",
        "#         x2 = self.bn2(x2)\n",
        "\n",
        "#         x3 = torch.cat([x1, x2], dim=1)\n",
        "#         x3 = self.dropout(F.elu(self.conv3(x3, edge_index)))\n",
        "#         x3 = self.bn3(x3)\n",
        "\n",
        "#         # Global pooling with attention to important nodes\n",
        "#         x_pool = global_mean_pool(x3, batch)\n",
        "\n",
        "#         # Final projection\n",
        "#         out = self.fc(x_pool)\n",
        "#         return out\n",
        "\n",
        "# # --- Set up model, optimizer, scheduler, and attribute matrix ---\n",
        "# model = ZSLGNN(input_dim=7).to(device)\n",
        "# attribute_matrix = AttributeMatrix(\n",
        "#     num_classes=len(LABEL_MAPPING),\n",
        "#     input_dim=100,        # dimension of your pretrained GloVe embeddings\n",
        "#     embed_dim=64,\n",
        "#     label_mapping=LABEL_MAPPING,\n",
        "#     pretrained_embeddings=pretrained_embeddings\n",
        "# ).to(device)\n",
        "\n",
        "# # Visualize attributes after defining attribute_matrix\n",
        "# visualize_attributes(attribute_matrix)\n",
        "\n",
        "# from torch.optim import AdamW\n",
        "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "# # Use separate learning rates for model and attribute matrix\n",
        "# model_params = list(model.parameters())\n",
        "# attr_params = list(attribute_matrix.parameters())\n",
        "\n",
        "# optimizer = AdamW([\n",
        "#     {'params': model_params, 'lr': 0.001, 'weight_decay': 0.01},\n",
        "#     {'params': attr_params, 'lr': 0.002, 'weight_decay': 0.005}  # Higher learning rate for attribute matrix\n",
        "# ])\n",
        "\n",
        "# # Use a more gradual scheduler\n",
        "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.0001)\n",
        "\n",
        "# # --- Data loaders ---\n",
        "# train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "# val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "# test_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# # --- Training function ---\n",
        "# def train(model, loader, optimizer, scheduler, attribute_matrix):\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "\n",
        "#     # Adjust class weights to emphasize trojan (class 3)\n",
        "#     class_weights = torch.tensor([1.0, 1.0, 1.0, 1.2, 1.5]).to(device)  # Increased weight for trojan\n",
        "\n",
        "#     margin = 0.3  # Increase margin for better separation\n",
        "\n",
        "#     for data in tqdm(loader, desc=\"Training\"):\n",
        "#         data = data.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         embeddings = model(data)\n",
        "\n",
        "#         # Get labels and corresponding attribute targets\n",
        "#         y_labels = data.y.squeeze() if data.y.dim() > 1 else data.y\n",
        "#         targets = attribute_matrix(y_labels)\n",
        "\n",
        "#         # Normalize embeddings and targets\n",
        "#         embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
        "#         targets_norm = targets  # Already normalized\n",
        "\n",
        "#         # Cosine distance per sample\n",
        "#         cosine_loss = 1 - F.cosine_similarity(embeddings_norm, targets_norm, dim=1)\n",
        "\n",
        "#         # A larger margin-based term for better separation\n",
        "#         margin_loss = torch.clamp(margin - F.cosine_similarity(embeddings_norm, targets_norm, dim=1), min=0.0)\n",
        "\n",
        "#         # Add focal loss component to focus on hard examples\n",
        "#         focal_weight = torch.exp(-F.cosine_similarity(embeddings_norm, targets_norm, dim=1))\n",
        "#         loss_elements = (cosine_loss + margin_loss) * focal_weight\n",
        "\n",
        "#         # Apply class weighting\n",
        "#         weighted_loss = loss_elements * class_weights[y_labels]\n",
        "\n",
        "#         # Add regularization term\n",
        "#         loss = weighted_loss.mean() + 0.01 * torch.norm(embeddings, p=2, dim=1).mean()\n",
        "\n",
        "#         loss.backward()\n",
        "\n",
        "#         # Gradient clipping to stabilize training\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     scheduler.step()\n",
        "#     return total_loss / len(loader)\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, f1_score\n",
        "\n",
        "# # --- Evaluation function ---\n",
        "# @torch.no_grad()\n",
        "# def evaluate(model, loader, attribute_matrix, bias=0.):\n",
        "#     model.eval()\n",
        "#     y_true, y_pred = [], []\n",
        "\n",
        "#     # Adjust class biases to correct for prediction imbalance\n",
        "#     # Increase bias for trojan (index 3) to make it more likely to be predicted\n",
        "#     class_biases = torch.zeros(len(LABEL_MAPPING)).to(device)\n",
        "#     class_biases[0] = 0.1  # Add bias to adware\n",
        "#     class_biases[1] = 0.1  # Add bias to benign\n",
        "#     class_biases[2] = 0.1  # Add bias to downloader\n",
        "#     # Leave trojan without additional bias\n",
        "#     class_biases[4] = 0.1  # Add bias to addisplay\n",
        "\n",
        "#     for data in tqdm(loader, desc=\"Evaluating\"):\n",
        "#         data = data.to(device)\n",
        "#         embeddings = model(data)\n",
        "#         y_labels = data.y.squeeze() if data.y.dim() > 1 else data.y\n",
        "#         embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
        "#         attr_norm = attribute_matrix()  # Full attribute matrix\n",
        "\n",
        "#         # Compute cosine similarity with class-specific bias adjustment\n",
        "#         sims = torch.mm(embeddings_norm, attr_norm.t())\n",
        "\n",
        "#         # Apply class-specific biases\n",
        "#         sims = sims + class_biases\n",
        "\n",
        "#         # Get predictions\n",
        "#         preds = sims.argmax(dim=1)\n",
        "\n",
        "#         y_true.extend(y_labels.cpu().tolist())\n",
        "#         y_pred.extend(preds.cpu().tolist())\n",
        "\n",
        "#     cm = confusion_matrix(y_true, y_pred)\n",
        "#     print(\"Confusion Matrix:\")\n",
        "#     print(cm)\n",
        "#     # Create a list of class names ordered by their numerical indices\n",
        "#     ordered_class_names = [k for k, v in sorted(LABEL_MAPPING.items(), key=lambda item: item[1])]\n",
        "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ordered_class_names)\n",
        "\n",
        "#     disp.plot(cmap=plt.cm.Blues)\n",
        "#     plt.title(\"Confusion Matrix\")\n",
        "#     plt.show()\n",
        "\n",
        "#     accuracy = accuracy_score(y_true, y_pred)\n",
        "#     f1 = f1_score(y_true, y_pred, average='macro')\n",
        "#     print(classification_report(y_true, y_pred))\n",
        "\n",
        "#     return accuracy, f1\n",
        "\n",
        "# # --- Main training loop ---\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(f\"Using device: {device}\")\n",
        "\n",
        "#     EPOCHS = 30\n",
        "#     best_val_acc = 0.0\n",
        "\n",
        "#     for epoch in range(EPOCHS):\n",
        "#         train_loss = train(model, train_loader, optimizer, scheduler, attribute_matrix)\n",
        "#         print(f\"Epoch {epoch + 1}/{EPOCHS}: Loss = {train_loss:.4f}\")\n",
        "\n",
        "#         # Evaluate on validation set every 3 epochs\n",
        "#         if (epoch + 1) % 3 == 0:\n",
        "#             val_acc, val_f1 = evaluate(model, val_loader, attribute_matrix)\n",
        "#             print(f\"Validation Accuracy: {val_acc:.4f}, Macro F1: {val_f1:.4f}\")\n",
        "#             if val_acc > best_val_acc:\n",
        "#                 best_val_acc = val_acc\n",
        "#                 # Save best model checkpoint\n",
        "#                 torch.save({\n",
        "#                     'model_state_dict': model.state_dict(),\n",
        "#                     'attribute_matrix_state_dict': attribute_matrix.state_dict(),\n",
        "#                     'optimizer_state_dict': optimizer.state_dict()\n",
        "#                 }, 'best_model.pth')\n",
        "\n",
        "#     print(\"\\nFinal Test Evaluation:\")\n",
        "#     test_acc, test_f1 = evaluate(model, test_loader, attribute_matrix)\n",
        "#     print(f\"Test Accuracy: {test_acc:.4f}, Macro F1: {test_f1:.4f}\")\n",
        "\n",
        "\n",
        "#     # Play audio after code execution\n",
        "#     youtube_url = \"https://www.youtube.com/watch?v=j9TykAvgnC4\"\n",
        "#     display(play_youtube_audio(youtube_url))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKitkLn8Br4g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lno_rnQp2YRr",
        "outputId": "072a7c9b-8d2f-434a-860b-a2d22d0e4612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spektral\n",
            "  Downloading spektral-1.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from spektral) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from spektral) (5.3.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from spektral) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from spektral) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from spektral) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from spektral) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from spektral) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from spektral) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from spektral) (4.67.1)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from spektral) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=2.2.0->spektral) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->spektral) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->spektral) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->spektral) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->spektral) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->spektral) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->spektral) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->spektral) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->spektral) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.2.0->spektral) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.2.0->spektral) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow>=2.2.0->spektral) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.2.0->spektral) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.2.0->spektral) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.2.0->spektral) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow>=2.2.0->spektral) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.2.0->spektral) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.2.0->spektral) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.2.0->spektral) (0.1.2)\n",
            "Downloading spektral-1.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spektral\n",
            "Successfully installed spektral-1.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install spektral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJqs259IvSLn",
        "outputId": "add9292d-dfa6-441e-f474-05e906659bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.9/171.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Downloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp, jedi\n",
            "Successfully installed jedi-0.19.2 yt-dlp-2025.2.19\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6fGQ4F4-n1A",
        "outputId": "21af4417-51b1-4bda-8197-a9c2263b8b11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-1c155e8a7641>:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_file, word2vec_file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion complete\n"
          ]
        }
      ],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_file = '/content/glove.6B.100d.txt' # Replace with the>\n",
        "word2vec_file = 'model.word2vec'\n",
        "glove2word2vec(glove_file, word2vec_file)\n",
        "print(\"Conversion complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VPAdO_X2ZaF"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUz_AQn35oAg",
        "outputId": "70bd9ccf-acb3-4637-a551-d3667d1696db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training graphs:   0%|          | 0/3500 [00:00<?, ?it/s]<ipython-input-6-642e9935f61f>:114: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  spectral_feature = torch.tensor([[np.mean(eigenvalues)]] * num_nodes, dtype=torch.float32)\n",
            "Training graphs:   1%|          | 25/3500 [01:41<4:52:30,  5.05s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import yt_dlp\n",
        "import IPython.display as ipd\n",
        "\n",
        "def play_youtube_audio(url):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'outtmpl': 'audio.%(ext)s'\n",
        "    }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "    return ipd.Audio(\"audio.mp3\", autoplay=True)\n",
        "\n",
        "# --- Set seeds and device ---\n",
        "SEED = 75\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Enable benchmark for faster training when possible\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Label and path definitions ---\n",
        "LABEL_MAPPING = {\n",
        "    'adware': 0,\n",
        "    'benign': 1,\n",
        "    'downloader': 2,\n",
        "    'trojan': 3,\n",
        "    'addisplay': 4  # This class will be treated as unseen during training\n",
        "}\n",
        "\n",
        "# Set file paths (modify if needed)\n",
        "graph_data_path = '/content/extracted_files/malnet/malnet-graphs-tiny'\n",
        "split_info_path = '/content/extracted_files/splitinfo/split_info_tiny/type'\n",
        "\n",
        "# --- Split load helper ---\n",
        "def load_split(file_name):\n",
        "    split_file = os.path.join(split_info_path, file_name)\n",
        "    with open(split_file, 'r') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "train_graphs = load_split('train.txt')\n",
        "val_graphs = load_split('val.txt')\n",
        "test_graphs = load_split('test.txt')\n",
        "\n",
        "# --- Node feature calculation ---\n",
        "@torch.no_grad()\n",
        "def calculate_node_features(G_nx, num_nodes):\n",
        "    # Basic centrality measures\n",
        "    degrees = torch.tensor([d for _, d in G_nx.degree()], dtype=torch.float32)\n",
        "    G_undirected = G_nx.to_undirected()\n",
        "    clustering_dict = nx.clustering(G_undirected)\n",
        "    clustering_values = torch.tensor([clustering_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "    # Calculate additional centrality metrics with size-based approximation\n",
        "    if num_nodes > 500:\n",
        "        betweenness_dict = nx.betweenness_centrality(G_nx, k=min(50, num_nodes))\n",
        "        closeness_dict = {i: 0.5 for i in range(num_nodes)}  # Default value for large graphs\n",
        "    else:\n",
        "        betweenness_dict = nx.betweenness_centrality(G_nx)\n",
        "        try:\n",
        "            closeness_dict = nx.closeness_centrality(G_nx)\n",
        "        except:\n",
        "            closeness_dict = {i: 0.5 for i in range(num_nodes)}\n",
        "\n",
        "    betweenness_values = torch.tensor([betweenness_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "    closeness_values = torch.tensor([closeness_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "    # Calculate eigenvector centrality (important for capturing influence patterns)\n",
        "    try:\n",
        "        eigenvector_dict = nx.eigenvector_centrality(G_nx, max_iter=100)\n",
        "    except Exception as e:\n",
        "        eigenvector_dict = {i: 1.0 / num_nodes for i in range(num_nodes)}\n",
        "    eigenvector_values = torch.tensor([eigenvector_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "    # Calculate pagerank (useful for identifying important nodes)\n",
        "    try:\n",
        "        pagerank_dict = nx.pagerank(G_nx, max_iter=100)\n",
        "    except:\n",
        "        pagerank_dict = {i: 1.0 / num_nodes for i in range(num_nodes)}\n",
        "    pagerank_values = torch.tensor([pagerank_dict[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "\n",
        "    # Include graph density feature (trojans often have distinct density patterns)\n",
        "    graph_density = nx.density(G_nx)\n",
        "    density_feature = torch.full((num_nodes, 1), graph_density, dtype=torch.float32)\n",
        "\n",
        "    # Add spectral features\n",
        "    try:\n",
        "        laplacian = nx.normalized_laplacian_matrix(G_undirected)\n",
        "        eigenvalues = np.linalg.eigvals(laplacian.toarray())[:5]  # Top 5 eigenvalues\n",
        "        spectral_feature = torch.tensor([[np.mean(eigenvalues)]] * num_nodes, dtype=torch.float32)\n",
        "    except:\n",
        "        spectral_feature = torch.zeros((num_nodes, 1), dtype=torch.float32)\n",
        "\n",
        "    # Combine all features\n",
        "    return torch.cat([\n",
        "        degrees.unsqueeze(1),\n",
        "        clustering_values.unsqueeze(1),\n",
        "        betweenness_values.unsqueeze(1),\n",
        "        eigenvector_values.unsqueeze(1),\n",
        "        closeness_values.unsqueeze(1),\n",
        "        pagerank_values.unsqueeze(1),\n",
        "        density_feature,\n",
        "        spectral_feature\n",
        "    ], dim=1)\n",
        "\n",
        "# --- Graph loading function with improved error handling ---\n",
        "def load_graphs(graph_list, desc=\"Loading graphs\"):\n",
        "    data_list = []\n",
        "    with tqdm(total=len(graph_list), desc=desc, position=0, leave=True) as pbar:\n",
        "        for graph_entry in graph_list:\n",
        "            # Expecting \"class/filename\" structure\n",
        "            category_folder, filename = os.path.split(graph_entry)\n",
        "            class_name = category_folder.split('/')[0]\n",
        "            if class_name not in LABEL_MAPPING:\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            graph_path = os.path.join(graph_data_path, category_folder, f\"{filename}.edgelist\")\n",
        "            if not os.path.exists(graph_path):\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Efficient edge list reading\n",
        "                with open(graph_path, 'r') as f:\n",
        "                    edge_list = [tuple(map(int, line.strip().split()))\n",
        "                                 for line in f if line.strip() and not line.startswith('#')]\n",
        "                if not edge_list:\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                edge_index = torch.tensor(edge_list).t().contiguous()\n",
        "                num_nodes = int(edge_index.max().item()) + 1\n",
        "\n",
        "                # Build networkx graph and calculate node features\n",
        "                G_nx = nx.DiGraph()\n",
        "                G_nx.add_nodes_from(range(num_nodes))\n",
        "                G_nx.add_edges_from(edge_list)\n",
        "                x = calculate_node_features(G_nx, num_nodes)\n",
        "\n",
        "                data = Data(\n",
        "                    x=x,\n",
        "                    edge_index=edge_index,\n",
        "                    y=torch.tensor([LABEL_MAPPING[class_name]], dtype=torch.long)\n",
        "                )\n",
        "                data_list.append(data)\n",
        "            except Exception as e:\n",
        "                print(f\"[Warning] Error processing {graph_path}: {str(e)}\")\n",
        "            finally:\n",
        "                pbar.update(1)\n",
        "    return data_list\n",
        "\n",
        "def visualize_attributes(attribute_matrix):\n",
        "    attr_vectors = attribute_matrix().detach().cpu().numpy()  # Detach the tensor to remove gradients\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=3)  # Set perplexity to 3 or another value less than 5\n",
        "    reduced = tsne.fit_transform(attr_vectors)\n",
        "    plt.scatter(reduced[:, 0], reduced[:, 1], c=range(len(LABEL_MAPPING)), cmap='viridis')\n",
        "    for i, label in enumerate(LABEL_MAPPING.keys()):\n",
        "        plt.annotate(label, (reduced[i, 0], reduced[i, 1]))\n",
        "    plt.title(\"t-SNE Visualization of Attribute Vectors\")\n",
        "    plt.show()\n",
        "\n",
        "# --- Load datasets ---\n",
        "print(\"Loading datasets...\")\n",
        "train_data_raw = load_graphs(train_graphs, desc=\"Training graphs\")\n",
        "val_data_raw = load_graphs(val_graphs, desc=\"Validation graphs\")\n",
        "test_data_raw = load_graphs(test_graphs, desc=\"Test graphs\")\n",
        "\n",
        "# Filter out the unseen class ('addisplay') from training data\n",
        "unseen_label = LABEL_MAPPING['addisplay']\n",
        "train_data = [d for d in train_data_raw if d.y.item() != unseen_label]\n",
        "val_data = val_data_raw\n",
        "test_data = test_data_raw\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# --- Load GloVe embeddings (pre-converted to Word2Vec format) ---\n",
        "glove_model = KeyedVectors.load_word2vec_format('/content/model.word2vec', binary=False)\n",
        "\n",
        "# Otherwise, try loading directly (GloVe format):\n",
        "# glove_model = KeyedVectors.load_word2vec_format('path/to/glove.6B.100d.txt', binary=False)\n",
        "# --- Build a dictionary of pretrained embeddings for each label ---\n",
        "pretrained_embeddings = {}\n",
        "for label in LABEL_MAPPING.keys():\n",
        "    if label in glove_model:\n",
        "        pretrained_embeddings[label] = glove_model[label].tolist()\n",
        "    else:\n",
        "        # If the label isn't found, create a random 100-d vector.\n",
        "        pretrained_embeddings[label] = [random.uniform(-0.1, 0.1) for _ in range(100)]\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn  # <-- add this at the top\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Attribute matrix module ---\n",
        "# --- Updated Attribute Matrix Module ---\n",
        "class AttributeMatrix(nn.Module):\n",
        "    def __init__(self, num_classes, input_dim, embed_dim, label_mapping, pretrained_embeddings):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Enhanced projection network with more layers\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(input_dim, embed_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        # Separate residual path\n",
        "        self.residual = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "        # Initialize embeddings with balanced weights\n",
        "        embeddings_list = []\n",
        "        for label, idx in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
        "            if label in pretrained_embeddings:\n",
        "                vec = torch.tensor(pretrained_embeddings[label], dtype=torch.float32)\n",
        "\n",
        "                # Give more weight to the trojan class embeddings but keep it balanced\n",
        "                if label == 'trojan':\n",
        "                    vec = vec * 1.05  # Reduced amplification for trojan features\n",
        "\n",
        "            else:\n",
        "                vec = torch.randn(input_dim)\n",
        "            embeddings_list.append(vec)\n",
        "\n",
        "        attributes_tensor = torch.stack(embeddings_list, dim=0)\n",
        "        projected = self.proj(attributes_tensor)\n",
        "        residual_out = self.residual(attributes_tensor)\n",
        "        combined = projected + residual_out  # Residual connection\n",
        "\n",
        "        # Initialize the attributes with the combined embeddings\n",
        "        self.attributes = nn.Parameter(combined)\n",
        "\n",
        "    def forward(self, indices=None):\n",
        "        # Add L2 normalization for better cosine similarity calculation\n",
        "        attr = F.normalize(self.attributes, p=2, dim=1)\n",
        "        return attr if indices is None else attr[indices]\n",
        "\n",
        "# --- Zero-Shot GNN Model ---\n",
        "class ZSLGNN(nn.Module):\n",
        "    def __init__(self, input_dim=7, hidden_dim=64, embed_dim=64, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        # Increase attention heads for better feature attention\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=4, dropout=dropout_rate)\n",
        "        self.conv2 = GATConv(hidden_dim * 4, hidden_dim, heads=4, dropout=dropout_rate)\n",
        "        self.conv3 = GATConv(hidden_dim * 4 * 2, hidden_dim, heads=4, dropout=dropout_rate)\n",
        "\n",
        "        # Node-level attention\n",
        "        self.node_attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Add batch normalization for more stable training\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim * 4)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim * 4)\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dim * 4)\n",
        "\n",
        "        # Enhanced MLP for final projection\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim * 2, embed_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x.to(device)\n",
        "        # Apply feature normalization\n",
        "        x = (x - x.mean(dim=0)) / (x.std(dim=0) + 1e-6)\n",
        "\n",
        "        edge_index = data.edge_index.to(device)\n",
        "        batch = data.batch.to(device) if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "        # Forward pass with skip connections and batch normalization\n",
        "        x1 = self.dropout(F.elu(self.conv1(x, edge_index)))\n",
        "        x1 = self.bn1(x1)\n",
        "\n",
        "        x2 = self.dropout(F.elu(self.conv2(x1, edge_index)))\n",
        "        x2 = self.bn2(x2)\n",
        "\n",
        "        x3 = torch.cat([x1, x2], dim=1)\n",
        "        x3_res = self.dropout(F.elu(self.conv3(x3, edge_index)))\n",
        "        # Residual connection with dimension matching\n",
        "        x3 = x3_res + x3[:, :x3_res.size(1)]\n",
        "        x3 = self.bn3(x3)\n",
        "\n",
        "        # Apply node attention\n",
        "        attention_weights = self.node_attention(x3)\n",
        "        x3 = x3 * attention_weights\n",
        "\n",
        "        # Global pooling\n",
        "        x_pool = global_mean_pool(x3, batch)\n",
        "\n",
        "        # Final projection\n",
        "        out = self.fc(x_pool)\n",
        "        return out\n",
        "\n",
        "\n",
        "# --- Set up model, optimizer, scheduler, and attribute matrix ---\n",
        "# Change this line\n",
        "model = ZSLGNN(input_dim=7).to(device)\n",
        "attribute_matrix = AttributeMatrix(\n",
        "    num_classes=len(LABEL_MAPPING),\n",
        "    input_dim=100,        # dimension of your pretrained GloVe embeddings\n",
        "    embed_dim=64,\n",
        "    label_mapping=LABEL_MAPPING,\n",
        "    pretrained_embeddings=pretrained_embeddings\n",
        ").to(device)\n",
        "\n",
        "# Visualize attributes after defining attribute_matrix\n",
        "visualize_attributes(attribute_matrix)\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "# Use separate learning rates for model and attribute matrix\n",
        "model_params = list(model.parameters())\n",
        "attr_params = list(attribute_matrix.parameters())\n",
        "\n",
        "optimizer = AdamW([\n",
        "    {'params': model_params, 'lr': 0.001, 'weight_decay': 0.01},\n",
        "    {'params': attr_params, 'lr': 0.002, 'weight_decay': 0.005}  # Higher learning rate for attribute matrix\n",
        "])\n",
        "\n",
        "# Use a more gradual scheduler\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.0001)\n",
        "\n",
        "# --- Data loaders ---\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# --- Training function ---\n",
        "def train(model, loader, optimizer, scheduler, attribute_matrix):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Balanced class weights\n",
        "    class_weights = torch.tensor([1.0, 1.0, 1.0, 1.2, 1.5]).to(device)\n",
        "\n",
        "    # Margin for better separation\n",
        "    margin = 0.3\n",
        "\n",
        "    for data in tqdm(loader, desc=\"Training\"):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = model(data)\n",
        "\n",
        "        # Get labels and corresponding attribute targets\n",
        "        y_labels = data.y.squeeze() if data.y.dim() > 1 else data.y\n",
        "        targets = attribute_matrix(y_labels)\n",
        "\n",
        "        # Normalize embeddings and targets\n",
        "        embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
        "        targets_norm = targets  # Already normalized\n",
        "\n",
        "        # Cosine distance per sample\n",
        "        cosine_loss = 1 - F.cosine_similarity(embeddings_norm, targets_norm, dim=1)\n",
        "\n",
        "        # Margin-based term for better separation\n",
        "        margin_loss = torch.clamp(margin - F.cosine_similarity(embeddings_norm, targets_norm, dim=1), min=0.0)\n",
        "\n",
        "        # Focal loss component to focus on hard examples\n",
        "        focal_weight = torch.exp(-F.cosine_similarity(embeddings_norm, targets_norm, dim=1))\n",
        "        loss_elements = (cosine_loss + margin_loss) * focal_weight\n",
        "\n",
        "        # Apply class weighting\n",
        "        weighted_loss = loss_elements * class_weights[y_labels]\n",
        "\n",
        "        # Add regularization term\n",
        "        loss = weighted_loss.mean() + 0.01 * torch.norm(embeddings, p=2, dim=1).mean()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to stabilize training\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, f1_score\n",
        "\n",
        "# --- Evaluation function ---\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, attribute_matrix, epoch=0):\n",
        "    model.eval()\n",
        "    y_true, y_pred = []\n",
        "    class_correct = {i: 0 for i in range(len(LABEL_MAPPING))}\n",
        "    class_total = {i: 0 for i in range(len(LABEL_MAPPING))}\n",
        "\n",
        "    # Dynamic temperature annealing\n",
        "    temperature = max(1.0, 1.5 - (epoch * 0.05))\n",
        "\n",
        "    # Balance class biases - adjust for addisplay and trojan\n",
        "    class_biases = torch.zeros(len(LABEL_MAPPING)).to(device)\n",
        "    class_biases[0] = 0.08  # Adware\n",
        "    class_biases[1] = 0.08  # Benign\n",
        "    class_biases[2] = 0.05  # Downloader (already performs well)\n",
        "    class_biases[3] = 0.05  # Small boost for trojan\n",
        "    class_biases[4] = 0.15  # Higher bias for addisplay to improve detection\n",
        "\n",
        "    for data in tqdm(loader, desc=\"Evaluating\"):\n",
        "        data = data.to(device)\n",
        "        embeddings = model(data)\n",
        "        y_labels = data.y.squeeze() if data.y.dim() > 1 else data.y\n",
        "        embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
        "        attr_norm = attribute_matrix()\n",
        "\n",
        "        # Apply temperature scaling before adding biases\n",
        "        sims = torch.mm(embeddings_norm, attr_norm.t()) / temperature\n",
        "\n",
        "        # Apply class biases after temperature scaling\n",
        "        sims = sims + class_biases\n",
        "\n",
        "        # Get predictions\n",
        "        preds = sims.argmax(dim=1)\n",
        "\n",
        "        # Track class-wise performance\n",
        "        for i in range(len(y_labels)):\n",
        "            label = y_labels[i].item()\n",
        "            pred = preds[i].item()\n",
        "            class_total[label] += 1\n",
        "            if pred == label:\n",
        "                class_correct[label] += 1\n",
        "\n",
        "        y_true.extend(y_labels.cpu().tolist())\n",
        "        y_pred.extend(preds.cpu().tolist())\n",
        "\n",
        "    # Display class-wise accuracy\n",
        "    print(\"\\nClass-wise accuracy:\")\n",
        "    for label_name, label_idx in LABEL_MAPPING.items():\n",
        "        accuracy = class_correct[label_idx] / max(1, class_total[label_idx])\n",
        "        print(f\"{label_name}: {accuracy:.2f}\")\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Display confusion matrix\n",
        "    ordered_class_names = [k for k, v in sorted(LABEL_MAPPING.items(), key=lambda item: item[1])]\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ordered_class_names)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    return accuracy, f1\n",
        "\n",
        "# --- Main training loop ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Training parameters\n",
        "    EPOCHS = 30\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_loss = train(model, train_loader, optimizer, scheduler, attribute_matrix)\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}: Loss = {train_loss:.4f}\")\n",
        "\n",
        "        # Evaluate more frequently\n",
        "        if (epoch + 1) % 2 == 0:  # Every 2 epochs\n",
        "            val_acc, val_f1 = evaluate(model, val_loader, attribute_matrix, epoch=epoch)\n",
        "            print(f\"Validation Accuracy: {val_acc:.4f}, Macro F1: {val_f1:.4f}\")\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_epoch = epoch\n",
        "                patience_counter = 0\n",
        "\n",
        "                # Save best model checkpoint\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'attribute_matrix_state_dict': attribute_matrix.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'val_acc': val_acc,\n",
        "                    'val_f1': val_f1\n",
        "                }, 'best_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}. Best epoch was {best_epoch+1} with validation accuracy {best_val_acc:.4f}\")\n",
        "                    break\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    checkpoint = torch.load('best_model.pth')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    attribute_matrix.load_state_dict(checkpoint['attribute_matrix_state_dict'])\n",
        "\n",
        "    print(\"\\nFinal Test Evaluation:\")\n",
        "    test_acc, test_f1 = evaluate(model, test_loader, attribute_matrix)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}, Macro F1: {test_f1:.4f}\")\n",
        "\n",
        "    # Play audio after code execution\n",
        "    youtube_url = \"https://www.youtube.com/watch?v=j9TykAvgnC4\"\n",
        "    display(play_youtube_audio(youtube_url))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "15w-wUYraaX2_APKQvUSh3c6EKsJiLXKB",
      "authorship_tag": "ABX9TyNqJRvTzWEAS7mtHhkhJR68",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}